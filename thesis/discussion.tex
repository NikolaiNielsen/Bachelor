\documentclass[main.tex]{subfiles}

\begin{document}
	\section{Discussion}
	In this thesis we set out to create a suite of programs to visualise key concept in the field of condensed matter physics. These concepts included crystal structures, families of lattice planes, a simulation of neutron scattering, along with band structures of two dimensional materials.
	
	The end result is a code package written in Python which accomplishes just that. It includes 4 main functions: \texttt{Lattice}, \texttt{Reciprocal}, \texttt{Scattering} and \texttt{Band\_structure}. These 4 functions take in a wealth of arguments, supplied by the user, and produce some sort of figure. Examples of these being the crystal structure of hexagonal lattice with a one atom basis (figure \ref{fig:lattice_demo_2}), the (001) family of lattice planes for a bcc lattice (figure \ref{fig:lattice_planes}), neutron scattering on an fcc lattice (figures \ref{fig:scattering_no_systemic} and \ref{fig:scattering_systemic}) or the band structure of a monovalent two dimensional material with a high strength potential (figure \ref{fig:band_structure_strong}).

	More can be done with these programs though, and they are by no means a comprehensive resource. For the lattice plotting program, there is currently no perfect algorithm to detect type of lattice that the user has input.
	
	The algorithm employed to detect lattices relies on the user inputting primitive lattice vectors with the specifications from appendix \ref{app:lattice}. These lattices can be rotated and scaled to the heart's desires, and the program will still detect them. But the algorithm does not take into account the non-uniqueness of primitive lattice vectors. 
	
	The "easy" way to take this into account would be to use the method specified in section \ref{sec:lattice_theory}, calculating the matrix $ M $ for the different lattices and checking whether or not it meets the necessary criteria. However, this method cannot work if the user also rotates the lattice. Different magnitudes of input primitive lattice vectors will also complicate things, as the program would have to check a range of different sets of primitive lattice vectors for each lattice. For example, say the user inputs an fcc lattice with $ a_1 = a \D(1,0,0) $, $ a_2 = a\D(0, 1/2, 1/2) $ and $ a_3 = a\D(1/2, 0, 1/2) $. There is currently no way for the program to "know" which magnitude to use for the primitive lattice vectors used for comparison, and the only recourse is to brute-force check all combinations.

	So currently no catch-all solution of classification is implemented. A thing to note is that the current algorithms only depend on the primitive lattice vectors. A different approach may be to construct the full crystal and look for a specific "type" of unit cell, corresponding to one of the Bravais lattices. However, this will necessarily involve a more complicated algorithm, which unfortunately there was not time for.
	
	Furthermore, the program only calculates the relevant quantities for one specific set of values per function call. As such, if the user wants to see how systemic absences appear in scattering experiments, or how the Fermi surface distorts for stronger potentials, they will need to manually call the relevant function multiple times with different form factors or potential strength. This gets tedious after a while.
	
	A solution to this is to create some sort of GUI which will display input boxes or sliders for the relevant quantities, automatically call the functions and display the results. This can be done, for example with the python package \href{http://flask.pocoo.org/}{Flask}, which allows the creation of web-applications. These can be run locally on the users machine and allows for javascript integration. This is especially useful (necessary, even!) for accomplishing the goal of added interactivity, as Matplotlib figures can be rendered as javascript objects.
	
	With regards to distortion of the Fermi surface: The next (small) step would be to maybe include different two dimensional lattices. A rectangular lattice would be as simple as altering a value or two in the code, for example. But perhaps a better change would be to bump up the dimensions of the lattices to three. Doing this, however, means we run into problems with the number of available dimensions. It seems we live in a universe with only three spatial dimensions, which means we will use up all of those just specifying the geometry of the lattice, leaving no dimension for any other quantities of interest - like the dispersion relation of the particles in the crystal.
	
	This limits us to only looking at isosurfaces of energy - like the Fermi surface, where we in two dimensions could view the entire Fermi sea (and the rest of the dispersion relation). While this would be  enough to view the distortion of the Fermi surface (especially if combined with a web-app as specified above), there is the added issue of the computational time required. Currently, for a two dimensional lattice, with $ n_k $ values of $ \V{k} $ in each direction, and $ n_G $ allowed values of $ \V{G} $ (again in each direction), necessitates finding the eigenvalues of $ n_k^2 $ matrices of size $ n_G^2 \times n_G^2 $. By default these values og $ n_k $ and $ n_G $ are around 100 and 7 respectively, meaning the program diagonalises $ 10^4 $ $ 49\times 49 $ matrices. Increasing the dimensionality would increase both the number of matrices to be diagonalised, and the size of these. We would increase the amount of matrices by a factor of $ n_k $, and the size of each of these would go from $ n_G^2 \times n_G^2$ to $ n_G^3 \times n_G^3 $. With the standard arguments this would necessitate the diagonalisation of $ 10^6 $ matrices of size $ 343 \times 343 $. A very considerable increase.
	
	As it currently stands the program does take a while to compute the band structure (around a second or so, for a decent resolution), and to get the same resolution in three dimensions would take at least a factor $ n_k \approx 100 $ longer (assuming it takes the same amount of time to diagonalise a $ 49 \times 49 $ matrix as it does a $ 343 \times 343 $ matrix, which it most certainly does not). So if three dimensions are to be considered, the program would need to be thoroughly optimised. One way to do this would be to employ an algorithm to find just the lowest eigenvalue of these matrices, as we are only concerned with the lowest band - i.e. the Fermi surface. 
	
	A line had to be drawn somewhere though, and a two dimensional lattice seemed like a reasonable compromise between added dimensionality and computational complexity.
	
	One last issue is in regards to the programming language and packages chosen. Python is an interpreted language, which means that it necessarily trades computational speed for added ease of development. There are alternatives to this, like \href{https://nim-lang.org/}{Nim} or \href{https://julialang.org/}{Julia}, which are properly compiled or just-in-time compiled respectively. They do not, however, have as large a community as Python does, and therefore not as big a support for third-party packages like Matplotlib.
	
	Further, Matplotlib has one glaring issue in that it does not support a fully fledged 3D graphics engine. This means that all the three dimensional figures in this thesis and the programs are actually just 2D projections of underlying 3D data (of course, computer screens are 2D, so the projection has to happen at some point, but the way Matplotlib does it has significant drawbacks). This creates artefacts like how there is no proper support for the intersection of surface plots. This can be seen in the band structure program, where the orange horizontal plane, indicating the Fermi energy, does not properly intersect the dispersion relation, seen in blue. 
	
	There are other packages, like \href{https://docs.enthought.com/mayavi/mayavi/}{Mayavi} which do support proper 3D plotting. However, these problems (and their potential solution in this package) were discovered too late in the process of writing, so could not be solved in time.
	
	In conclusion: 4 programs have been created which illustrate concepts in the field of condensed matter physics. While they are not without their flaws or potential for improvement, they do still hold merit, and could be a valuable tool if used in combination with traditional book-based learning.
	
\end{document}